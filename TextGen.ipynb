{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MasakhaNERDataset.__init__() got an unexpected keyword argument 'trust_remote_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the dataset for Yorùbá with trust_remote_code set to True\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMasakhaNERDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: MasakhaNERDataset.__init__() got an unexpected keyword argument 'trust_remote_code'"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset for Yorùbá with trust_remote_code set to True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Alexie L/AppData/Local/Programs/Python/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources for tokenization\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SumXLDaset.__init__() got an unexpected keyword argument 'trust_remote_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data for all specified languages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize the dataset with a specific sample size (e.g., 2 for testing)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSumXLDaset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Print the training data for one of the languages, e.g., English\u001b[39;00m\n\u001b[0;32m     50\u001b[0m pprint(dataset\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: SumXLDaset.__init__() got an unexpected keyword argument 'trust_remote_code'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import datasets\n",
    "\n",
    "class SumXLDaset:\n",
    "    def __init__(self, sample_size):\n",
    "        self.all_data = {}\n",
    "        self.train_data = {}\n",
    "        self.valid_data = {}\n",
    "        self.test_data = {}\n",
    "        self.sample_size = sample_size\n",
    "        self.LANGS = [\n",
    "            \"amharic\", \"arabic\", \"azerbaijani\", \"bengali\", \"burmese\",\n",
    "            \"chinese_simplified\", \"chinese_traditional\", \"english\", \"french\", \n",
    "            \"gujarati\", \"hausa\", \"hindi\", \"igbo\", \"indonesian\", \"japanese\",\n",
    "            \"kirundi\", \"korean\", \"kyrgyz\", \"marathi\", \"nepali\", \"oromo\", \n",
    "            \"pashto\", \"persian\", \"pidgin\", \"portuguese\", \"punjabi\", \"russian\", \n",
    "            \"scottish_gaelic\", \"serbian_cyrillic\", \"serbian_latin\", \"sinhala\", \n",
    "            \"somali\", \"spanish\", \"swahili\", \"tamil\", \"telugu\", \"thai\", \n",
    "            \"tigrinya\", \"turkish\", \"ukrainian\", \"urdu\", \"uzbek\", \"vietnamese\", \n",
    "            \"welsh\", \"yoruba\"\n",
    "        ]\n",
    "        \n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        for lang in self.LANGS:\n",
    "            try:\n",
    "                # Load the dataset for each language with trust_remote_code=True\n",
    "                dataset = datasets.load_dataset('GEM/xlsum', lang, trust_remote_code=True)\n",
    "                \n",
    "                # Load samples based on sample_size argument\n",
    "                if self.sample_size > 0:\n",
    "                    self.train_data[lang] = dataset['train'].select(range(min(self.sample_size, len(dataset['train']))))\n",
    "                    self.valid_data[lang] = dataset['validation'].select(range(min(self.sample_size, len(dataset['validation']))))\n",
    "                    self.test_data[lang] = dataset['test'].select(range(min(self.sample_size, len(dataset['test']))))\n",
    "                else:\n",
    "                    self.train_data[lang] = dataset['train']\n",
    "                    self.valid_data[lang] = dataset['validation']\n",
    "                    self.test_data[lang] = dataset['test']\n",
    "                    \n",
    "                print(f\"Loaded data for language: {lang}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data for language {lang}: {e}\")\n",
    "                \n",
    "        print(f\"Loaded data for all specified languages.\")\n",
    "\n",
    "# Initialize the dataset with a specific sample size (e.g., 2 for testing)\n",
    "dataset = SumXLDaset(sample_size=2)\n",
    "\n",
    "# Print the training data for one of the languages, e.g., English\n",
    "pprint(dataset.train_data['english'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice file saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Sample hypotheses and references for testing\n",
    "hyps = [\"The quick brown fox jumps over the lazy dog\", \"A journey of a thousand miles begins with a single step\"]\n",
    "refs = [\"The quick brown fox leaped over a lazy dog\", \"A journey of a thousand miles starts with one step\"]\n",
    "\n",
    "# Function to calculate Distinct-1 and Distinct-2\n",
    "def distinct_n_grams(text, n):\n",
    "    n_grams = set(zip(*[text[i:] for i in range(n)]))\n",
    "    return len(n_grams) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "# Function to evaluate generation metrics\n",
    "def evaluate_generation_metrics(hyps, refs):\n",
    "    distinct_1_scores = []\n",
    "    distinct_2_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate ROUGE and Distinct metrics\n",
    "    for hyp, ref in zip(hyps, refs):\n",
    "        rouge_scores = rouge.score(ref, hyp)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "        distinct_1 = distinct_n_grams(hyp.split(), 1)\n",
    "        distinct_2 = distinct_n_grams(hyp.split(), 2)\n",
    "        distinct_1_scores.append(distinct_1)\n",
    "        distinct_2_scores.append(distinct_2)\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    P, R, F1 = bert_score(hyps, refs, lang='en', rescale_with_baseline=False)\n",
    "    print(\"BERTScore calculated.\")\n",
    "\n",
    "    # Organize report dictionary\n",
    "    report_dict = {\n",
    "        \"ROUGE-1\": np.mean(rouge1_scores),\n",
    "        \"ROUGE-2\": np.mean(rouge2_scores),\n",
    "        \"ROUGE-L\": np.mean(rougeL_scores),\n",
    "        \"BERTScore (P)\": np.mean(P),\n",
    "        \"BERTScore (R)\": np.mean(R),\n",
    "        \"BERTScore (F1)\": np.mean(F1),\n",
    "        \"Distinct-1\": np.mean(distinct_1_scores),\n",
    "        \"Distinct-2\": np.mean(distinct_2_scores),\n",
    "        \"Ensemble\": np.mean([np.mean(rouge1_scores), np.mean(rouge2_scores), np.mean(rougeL_scores), np.mean(F1)])\n",
    "    }\n",
    "\n",
    "    return report_dict\n",
    "\n",
    "# Evaluate and display metrics\n",
    "print(\"Evaluating generation metrics...\")\n",
    "report = evaluate_generation_metrics(hyps, refs)\n",
    "print(\"Evaluation Report:\")\n",
    "print(json.dumps(report, indent=4))\n",
    "\n",
    "# Save the evaluation report to JSON\n",
    "output_path = \"evaluation_report_test.json\"\n",
    "with open(output_path, \"w\") as outfile:\n",
    "    json.dump(report, outfile, indent=4)\n",
    "\n",
    "print(f\"Report saved successfully to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
