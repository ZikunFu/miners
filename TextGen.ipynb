{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MasakhaNERDataset.__init__() got an unexpected keyword argument 'trust_remote_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize the dataset for Yorùbá with trust_remote_code set to True\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mMasakhaNERDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: MasakhaNERDataset.__init__() got an unexpected keyword argument 'trust_remote_code'"
     ]
    }
   ],
   "source": [
    "# Initialize the dataset for Yorùbá with trust_remote_code set to True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.12.4' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '\"c:/Users/Alexie L/AppData/Local/Programs/Python/Python312/python.exe\" -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK resources for tokenization\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SumXLDaset.__init__() got an unexpected keyword argument 'trust_remote_code'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded data for all specified languages.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Initialize the dataset with a specific sample size (e.g., 2 for testing)\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mSumXLDaset\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Print the training data for one of the languages, e.g., English\u001b[39;00m\n\u001b[0;32m     50\u001b[0m pprint(dataset\u001b[38;5;241m.\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: SumXLDaset.__init__() got an unexpected keyword argument 'trust_remote_code'"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "import datasets\n",
    "\n",
    "class SumXLDaset:\n",
    "    def __init__(self, sample_size):\n",
    "        self.all_data = {}\n",
    "        self.train_data = {}\n",
    "        self.valid_data = {}\n",
    "        self.test_data = {}\n",
    "        self.sample_size = sample_size\n",
    "        self.LANGS = [\n",
    "            \"amharic\", \"arabic\", \"azerbaijani\", \"bengali\", \"burmese\",\n",
    "            \"chinese_simplified\", \"chinese_traditional\", \"english\", \"french\", \n",
    "            \"gujarati\", \"hausa\", \"hindi\", \"igbo\", \"indonesian\", \"japanese\",\n",
    "            \"kirundi\", \"korean\", \"kyrgyz\", \"marathi\", \"nepali\", \"oromo\", \n",
    "            \"pashto\", \"persian\", \"pidgin\", \"portuguese\", \"punjabi\", \"russian\", \n",
    "            \"scottish_gaelic\", \"serbian_cyrillic\", \"serbian_latin\", \"sinhala\", \n",
    "            \"somali\", \"spanish\", \"swahili\", \"tamil\", \"telugu\", \"thai\", \n",
    "            \"tigrinya\", \"turkish\", \"ukrainian\", \"urdu\", \"uzbek\", \"vietnamese\", \n",
    "            \"welsh\", \"yoruba\"\n",
    "        ]\n",
    "        \n",
    "        self.load_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        for lang in self.LANGS:\n",
    "            try:\n",
    "                # Load the dataset for each language with trust_remote_code=True\n",
    "                dataset = datasets.load_dataset('GEM/xlsum', lang, trust_remote_code=True)\n",
    "                \n",
    "                # Load samples based on sample_size argument\n",
    "                if self.sample_size > 0:\n",
    "                    self.train_data[lang] = dataset['train'].select(range(min(self.sample_size, len(dataset['train']))))\n",
    "                    self.valid_data[lang] = dataset['validation'].select(range(min(self.sample_size, len(dataset['validation']))))\n",
    "                    self.test_data[lang] = dataset['test'].select(range(min(self.sample_size, len(dataset['test']))))\n",
    "                else:\n",
    "                    self.train_data[lang] = dataset['train']\n",
    "                    self.valid_data[lang] = dataset['validation']\n",
    "                    self.test_data[lang] = dataset['test']\n",
    "                    \n",
    "                print(f\"Loaded data for language: {lang}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading data for language {lang}: {e}\")\n",
    "                \n",
    "        print(f\"Loaded data for all specified languages.\")\n",
    "\n",
    "# Initialize the dataset with a specific sample size (e.g., 2 for testing)\n",
    "dataset = SumXLDaset(sample_size=2)\n",
    "\n",
    "# Print the training data for one of the languages, e.g., English\n",
    "pprint(dataset.train_data['english'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert-score\n",
      "  Using cached bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.3.1)\n",
      "Collecting pandas>=1.0.1 (from bert-score)\n",
      "  Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers>=3.0.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.46.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (1.26.4)\n",
      "Requirement already satisfied: requests in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (4.66.6)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (3.9.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bert-score) (24.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.0.1->bert-score)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.0.1->bert-score)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.15.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.0.0->bert-score) (2021.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\alexie l\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.31.1->bert-score) (0.4.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (2024.9.11)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers>=3.0.0->bert-score) (0.20.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (4.53.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib->bert-score) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->bert-score) (2024.8.30)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.0.0->bert-score) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.0.0->bert-score) (2021.12.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert-score) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\alexie l\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
      "Using cached bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "Downloading pandas-2.2.3-cp312-cp312-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.5 MB 1.3 MB/s eta 0:00:09\n",
      "   - -------------------------------------- 0.5/11.5 MB 5.8 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.1/11.5 MB 9.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 2.0/11.5 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 3.0/11.5 MB 13.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 4.4/11.5 MB 15.6 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.1/11.5 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 8.1/11.5 MB 21.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 28.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.5 MB 34.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 29.8 MB/s eta 0:00:00\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas, bert-score\n",
      "Successfully installed bert-score-0.3.13 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install bert-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice file saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating generation metrics...\n",
      "Calculating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore calculated.\n",
      "Evaluation Report:\n",
      "{\n",
      "    \"ROUGE-1\": 0.7698412698412699,\n",
      "    \"ROUGE-2\": 0.513157894736842,\n",
      "    \"ROUGE-L\": 0.7698412698412699,\n",
      "    \"BERTScore (P)\": 0.9830428957939148,\n",
      "    \"BERTScore (R)\": 0.9846949875354767,\n",
      "    \"BERTScore (F1)\": 0.983854204416275,\n",
      "    \"Distinct-1\": 0.9545454545454546,\n",
      "    \"Distinct-2\": 0.898989898989899,\n",
      "    \"Ensemble\": 0.7591736597089142\n",
      "}\n",
      "Report saved successfully to evaluation_report_test.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Sample hypotheses and references for testing\n",
    "hyps = [\"The quick brown fox jumps over the lazy dog\", \"A journey of a thousand miles begins with a single step\"]\n",
    "refs = [\"The quick brown fox leaped over a lazy dog\", \"A journey of a thousand miles starts with one step\"]\n",
    "\n",
    "# Function to calculate Distinct-1 and Distinct-2\n",
    "def distinct_n_grams(text, n):\n",
    "    n_grams = set(zip(*[text[i:] for i in range(n)]))\n",
    "    return len(n_grams) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "# Function to evaluate generation metrics\n",
    "def evaluate_generation_metrics(hyps, refs):\n",
    "    distinct_1_scores = []\n",
    "    distinct_2_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate ROUGE and Distinct metrics\n",
    "    for hyp, ref in zip(hyps, refs):\n",
    "        rouge_scores = rouge.score(ref, hyp)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "        distinct_1 = distinct_n_grams(hyp.split(), 1)\n",
    "        distinct_2 = distinct_n_grams(hyp.split(), 2)\n",
    "        distinct_1_scores.append(distinct_1)\n",
    "        distinct_2_scores.append(distinct_2)\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    P, R, F1 = bert_score(hyps, refs, lang='en', rescale_with_baseline=False)\n",
    "    print(\"BERTScore calculated.\")\n",
    "\n",
    "    # Convert tensors to numpy arrays, then to Python floats\n",
    "    P, R, F1 = P.cpu().numpy().astype(float), R.cpu().numpy().astype(float), F1.cpu().numpy().astype(float)\n",
    "\n",
    "    # Organize report dictionary\n",
    "    report_dict = {\n",
    "        \"ROUGE-1\": float(np.mean(rouge1_scores)),\n",
    "        \"ROUGE-2\": float(np.mean(rouge2_scores)),\n",
    "        \"ROUGE-L\": float(np.mean(rougeL_scores)),\n",
    "        \"BERTScore (P)\": float(np.mean(P)),\n",
    "        \"BERTScore (R)\": float(np.mean(R)),\n",
    "        \"BERTScore (F1)\": float(np.mean(F1)),\n",
    "        \"Distinct-1\": float(np.mean(distinct_1_scores)),\n",
    "        \"Distinct-2\": float(np.mean(distinct_2_scores)),\n",
    "        \"Ensemble\": float(np.mean([np.mean(rouge1_scores), np.mean(rouge2_scores), np.mean(rougeL_scores), np.mean(F1)]))\n",
    "    }\n",
    "\n",
    "    return report_dict\n",
    "\n",
    "# Evaluate and display metrics\n",
    "print(\"Evaluating generation metrics...\")\n",
    "report = evaluate_generation_metrics(hyps, refs)\n",
    "print(\"Evaluation Report:\")\n",
    "print(json.dumps(report, indent=4))\n",
    "\n",
    "# Save the evaluation report to JSON\n",
    "output_path = \"evaluation_report_test.json\"\n",
    "with open(output_path, \"w\") as outfile:\n",
    "    json.dump(report, outfile, indent=4)\n",
    "\n",
    "print(f\"Report saved successfully to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MasakhaNERDataset\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, AutoModelForCausalLM\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import argparse\n",
    "import json\n",
    "import hashlib\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from utils import MasakhaNERDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from seqeval.metrics import classification_report\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import numpy as np\n",
    "from transformers.utils import logging\n",
    "\n",
    "\n",
    "\n",
    "logging.set_verbosity_error() \n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "\n",
    "OPENAI_TOKEN = \"\"\n",
    "COHERE_TOKEN = \"\"\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "def argmax(array):\n",
    "    \"\"\"argmax with deterministic pseudorandom tie breaking.\"\"\"\n",
    "    max_indices = np.arange(len(array))[array == np.max(array)]\n",
    "    idx = int(hashlib.sha256(np.asarray(array).tobytes()).hexdigest(),16) % len(max_indices)\n",
    "    return max_indices[idx]\n",
    "\n",
    "def logsumexp(x):\n",
    "    c = x.max()\n",
    "    return c + np.log(np.sum(np.exp(x - c)))\n",
    "\n",
    "def normalize(x):\n",
    "    x = np.array(x)\n",
    "    return np.exp(x - logsumexp(x))\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def get_llama3_instruct_chat_response(gen_model, tokenizer, gen_model_checkpoint, messages, seed,verbose=True):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(gen_model.device)\n",
    "\n",
    "    terminators = [\n",
    "        tokenizer.eos_token_id,\n",
    "        tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = gen_model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=64,  \n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        top_p=1\n",
    "    )\n",
    "    inputs = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
    "    response = outputs[0][input_ids.shape[-1]:]\n",
    "    response = tokenizer.decode(response, skip_special_tokens=True)\n",
    "    if(verbose):\n",
    "        print(\"\\n\"+\"=\"*35+\"INPUT\"+\"=\"*35)\n",
    "        print(inputs)\n",
    "        print(\"=\"*35+\"RESPONSE\"+\"=\"*43)\n",
    "        print(response)\n",
    "        print(\"=\"*70)\n",
    "    return response\n",
    "\n",
    "def retrieve_ids(train_embeddings, test_embeddings, train_labels, k, balance=False, all_possible_labels=[]):\n",
    "    all_samples = []\n",
    "    for test_id in tqdm(range(len(test_embeddings))):\n",
    "        dists = []\n",
    "        batch_size = 1                              ########change back to 128 \n",
    "        if len(train_embeddings) < batch_size:\n",
    "            batch_size = len(test_embeddings) // 2\n",
    "        \n",
    "        num_of_batches = len(train_embeddings) // batch_size\n",
    "\n",
    "        if (len(train_embeddings) % batch_size) > 0:\n",
    "            num_of_batches += 1\n",
    "\n",
    "        for i in range(num_of_batches):\n",
    "            train_embedding = torch.FloatTensor(train_embeddings[i*batch_size:(i+1)*batch_size]).unsqueeze(1).cuda()\n",
    "            \n",
    "            test_embedding = torch.FloatTensor(test_embeddings[test_id]).unsqueeze(0)\n",
    "            test_embedding = test_embedding.expand(len(train_embedding), -1).unsqueeze(1).cuda()\n",
    "            \n",
    "            dist = torch.cdist(test_embedding, train_embedding, p=2, compute_mode='use_mm_for_euclid_dist_if_necessary').squeeze().tolist()\n",
    "\n",
    "            if isinstance(dist, float):\n",
    "                dist = [dist]\n",
    "\n",
    "            for j in range(len(dist)):\n",
    "                dists.append([dist[j], train_labels[i*batch_size + j], i*batch_size + j])\n",
    "\n",
    "        if balance:\n",
    "            sorted_dists = sorted(dists, key=lambda l: l[0], reverse=False)\n",
    "        else:\n",
    "            sorted_dists = sorted(dists, key=lambda l: l[0], reverse=False)[:k]\n",
    "\n",
    "        all_indices = []\n",
    "        if balance:\n",
    "            for opt in all_possible_labels:\n",
    "                count_found = 0\n",
    "                for obj in sorted_dists:\n",
    "                    if opt == obj[1]:\n",
    "                        all_indices.append(obj[2])\n",
    "                        count_found += 1\n",
    "                        if count_found == k:\n",
    "                            break\n",
    "        else:\n",
    "            all_indices = [obj[2] for obj in sorted_dists]\n",
    "        all_samples.append(all_indices)\n",
    "    return all_samples\n",
    "\n",
    "def construct_prompt(few_shot_examples, test_tokens):\n",
    "    messages = []\n",
    "    system_message = {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful assistant that performs open ended generation and outputs completed prompts in multiple languages such as: english, french, afgan, german\"\n",
    "    }\n",
    "                                                                         \n",
    "    messages.append(system_message)\n",
    "    \n",
    "    for tokens, ner_tags in few_shot_examples:\n",
    "        temp = f'''\n",
    "        finish these prompts: \n",
    "        \"once upon a time\"\n",
    "        \"Describe a day in the life of a lighthouse keeper on a remote island.\"\n",
    "        \"What do you think the future of transportation will look like in 50 years?\"\n",
    "        \"Imagine you are a traveler in ancient China. What sights and experiences would you have?\"\n",
    "        '''\n",
    "        user_message = {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": temp\n",
    "        }\n",
    "        messages.append(user_message)\n",
    "        assistant_message = {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \" \".join(dataset.convert_ner_tags(ner_tags, to_BIO=True))\n",
    "        }\n",
    "        messages.append(assistant_message)\n",
    "    \n",
    "    temp = f'''\n",
    "         finish these prompts: \n",
    "        \"once upon a time\"\n",
    "        \"Describe a day in the life of a lighthouse keeper on a remote island.\"\n",
    "        '''\n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": temp    \n",
    "        }\n",
    "    messages.append(user_message)\n",
    "    return messages\n",
    "\n",
    "\n",
    "    pred_labels = output.strip().split()\n",
    "    if len(pred_labels) < num_tokens:\n",
    "        pred_labels.extend(['O'] * (num_tokens - len(pred_labels)))\n",
    "    elif len(pred_labels) > num_tokens:\n",
    "        pred_labels = pred_labels[:num_tokens]\n",
    "    return pred_labels\n",
    "\n",
    "def convert_numpy_types(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_types(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_types(v) for v in obj]\n",
    "    elif isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Define additional helper functions\n",
    "def distinct_n_grams(text, n):\n",
    "    n_grams = set(zip(*[text[i:] for i in range(n)]))\n",
    "    return len(n_grams) / len(text) if len(text) > 0 else 0\n",
    "\n",
    "def process_model_output(output, num_tokens):\n",
    "    pred_labels = output.strip().split()\n",
    "    if len(pred_labels) < num_tokens:\n",
    "        pred_labels.extend(['O'] * (num_tokens - len(pred_labels)))\n",
    "    elif len(pred_labels) > num_tokens:\n",
    "        pred_labels = pred_labels[:num_tokens]\n",
    "    return pred_labels\n",
    "\n",
    "# Evaluation Metrics Function\n",
    "def evaluate_generation_metrics(hyps, refs):\n",
    "    distinct_1_scores = []\n",
    "    distinct_2_scores = []\n",
    "    rouge1_scores = []\n",
    "    rouge2_scores = []\n",
    "    rougeL_scores = []\n",
    "\n",
    "    # Initialize ROUGE scorer\n",
    "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    # Calculate ROUGE and Distinct metrics\n",
    "    for hyp, ref in zip(hyps, refs):\n",
    "        rouge_scores = rouge.score(ref, hyp)\n",
    "        rouge1_scores.append(rouge_scores['rouge1'].fmeasure)\n",
    "        rouge2_scores.append(rouge_scores['rouge2'].fmeasure)\n",
    "        rougeL_scores.append(rouge_scores['rougeL'].fmeasure)\n",
    "\n",
    "        distinct_1 = distinct_n_grams(hyp.split(), 1)\n",
    "        distinct_2 = distinct_n_grams(hyp.split(), 2)\n",
    "        distinct_1_scores.append(distinct_1)\n",
    "        distinct_2_scores.append(distinct_2)\n",
    "\n",
    "    # Calculate BERTScore\n",
    "    print(\"Calculating BERTScore...\")\n",
    "    P, R, F1 = bert_score(hyps, refs, lang='en', rescale_with_baseline=False)\n",
    "    print(\"BERTScore calculated.\")\n",
    "\n",
    "    # Convert tensors to Python floats\n",
    "    P, R, F1 = P.cpu().numpy().astype(float), R.cpu().numpy().astype(float), F1.cpu().numpy().astype(float)\n",
    "\n",
    "    # Organize report dictionary\n",
    "    report_dict = {\n",
    "        \"ROUGE-1\": float(np.mean(rouge1_scores)),\n",
    "        \"ROUGE-2\": float(np.mean(rouge2_scores)),\n",
    "        \"ROUGE-L\": float(np.mean(rougeL_scores)),\n",
    "        \"BERTScore (P)\": float(np.mean(P)),\n",
    "        \"BERTScore (R)\": float(np.mean(R)),\n",
    "        \"BERTScore (F1)\": float(np.mean(F1)),\n",
    "        \"Distinct-1\": float(np.mean(distinct_1_scores)),\n",
    "        \"Distinct-2\": float(np.mean(distinct_2_scores)),\n",
    "        \"Ensemble\": float(np.mean([np.mean(rouge1_scores), np.mean(rouge2_scores), np.mean(rougeL_scores), np.mean(F1)]))\n",
    "    }\n",
    "\n",
    "    return report_dict\n",
    "\n",
    "# Main function setup\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--model_checkpoint\", default=\"sentence-transformers/LaBSE\", type=str, help=\"Path to embedding model\")\n",
    "    parser.add_argument(\"--gen_model_checkpoint\", default=\"meta-llama/Meta-Llama-3.1-8B-Instruct\", type=str, help=\"Generation model\")\n",
    "    parser.add_argument(\"--dataset\", type=str, default=\"masakhaner\", help=\"Dataset name\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42, help=\"Random seed for initialization\")\n",
    "    parser.add_argument(\"--cuda\", action=\"store_true\", help=\"Use CUDA when available\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "    # Load embedding and generation models\n",
    "    embedding_model = SentenceTransformer(args.model_checkpoint).cuda() if args.cuda else SentenceTransformer(args.model_checkpoint)\n",
    "    gen_model = AutoModelForCausalLM.from_pretrained(args.gen_model_checkpoint).cuda() if args.cuda else AutoModelForCausalLM.from_pretrained(args.gen_model_checkpoint)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.gen_model_checkpoint)\n",
    "\n",
    "    # Placeholder dataset - replace this section with actual data loading\n",
    "    hyps = [\"The quick brown fox jumps over the lazy dog\", \"A journey of a thousand miles begins with a single step\"]\n",
    "    refs = [\"The quick brown fox leaped over a lazy dog\", \"A journey of a thousand miles starts with one step\"]\n",
    "\n",
    "    # Evaluate and display metrics\n",
    "    print(\"Evaluating generation metrics...\")\n",
    "    report = evaluate_generation_metrics(hyps, refs)\n",
    "    print(\"Evaluation Report:\")\n",
    "    print(json.dumps(report, indent=4))\n",
    "\n",
    "    # Save report to JSON file\n",
    "    output_dir = \"logs/save_icl_NER\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, \"evaluation_report.json\")\n",
    "    with open(output_path, \"w\") as outfile:\n",
    "        json.dump(report, outfile, indent=4)\n",
    "    print(f\"Report saved successfully to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
